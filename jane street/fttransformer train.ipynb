{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":10253875,"sourceType":"datasetVersion","datasetId":6297065},{"sourceId":215383590,"sourceType":"kernelVersion"},{"sourceId":215384735,"sourceType":"kernelVersion"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tab-transformer-pytorch -q --no-index --find-links=/kaggle/input/jane-street-import/tab-transformer-pytorch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:15:28.327949Z","iopub.execute_input":"2024-12-31T09:15:28.328218Z","iopub.status.idle":"2024-12-31T09:15:32.832677Z","shell.execute_reply.started":"2024-12-31T09:15:28.328192Z","shell.execute_reply":"2024-12-31T09:15:32.831629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install hyper_connections","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:15:32.833752Z","iopub.execute_input":"2024-12-31T09:15:32.834102Z","iopub.status.idle":"2024-12-31T09:15:36.210242Z","shell.execute_reply.started":"2024-12-31T09:15:32.834072Z","shell.execute_reply":"2024-12-31T09:15:36.209419Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# load Data","metadata":{}},{"cell_type":"code","source":"import os,gc\nimport pickle\nimport polars as pl\nimport numpy as np\nimport pandas as pd\nimport joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:15:36.211216Z","iopub.execute_input":"2024-12-31T09:15:36.211445Z","iopub.status.idle":"2024-12-31T09:15:36.730264Z","shell.execute_reply.started":"2024-12-31T09:15:36.211427Z","shell.execute_reply":"2024-12-31T09:15:36.729384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reduce_mem_usage(df, float16_as32=True):\n    #memory_usage()是df每列的内存使用量,sum是对它们求和, B->KB->MB\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:#遍历每列的列名\n        col_type = df[col].dtype#列名的type\n        if col_type != object and str(col_type)!='category':#不是object也就是说这里处理的是数值类型的变量\n            c_min,c_max = df[col].min(),df[col].max() #求出这列的最大值和最小值\n            if str(col_type)[:3] == 'int':#如果是int类型的变量,不管是int8,int16,int32还是int64\n                #如果这列的取值范围是在int8的取值范围内,那就对类型进行转换 (-128 到 127)\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                #如果这列的取值范围是在int16的取值范围内,那就对类型进行转换(-32,768 到 32,767)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                #如果这列的取值范围是在int32的取值范围内,那就对类型进行转换(-2,147,483,648到2,147,483,647)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                #如果这列的取值范围是在int64的取值范围内,那就对类型进行转换(-9,223,372,036,854,775,808到9,223,372,036,854,775,807)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:#如果是浮点数类型.\n                #如果数值在float16的取值范围内,如果觉得需要更高精度可以考虑float32\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    if float16_as32:#如果数据需要更高的精度可以选择float32\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float16)  \n                #如果数值在float32的取值范围内，对它进行类型转换\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                #如果数值在float64的取值范围内，对它进行类型转换\n                else:\n                    df[col] = df[col].astype(np.float64)\n    #计算一下结束后的内存\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #相比一开始的内存减少了百分之多少\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:15:36.731285Z","iopub.execute_input":"2024-12-31T09:15:36.731714Z","iopub.status.idle":"2024-12-31T09:15:36.740644Z","shell.execute_reply.started":"2024-12-31T09:15:36.731683Z","shell.execute_reply":"2024-12-31T09:15:36.739727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CONFIG():\n    def __init__(self):\n        self.train_data_path = '/kaggle/input/data-create-create-lags/training.parquet'\n        self.valid_data_path = '/kaggle/input/data-create-create-lags/validation.parquet'\n        self.feature_names = [f\"feature_{i:02d}\" for i in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n        self.label_name = 'responder_6'\n        self.weight_name = 'weight'\n        self.feature_cat = [\"feature_09\", \"feature_10\", \"feature_11\"]\n        self.feature_cont = [item for item in self.feature_names if item not in self.feature_cat]\n        self.train_start_dt = 1100\nmy_config = CONFIG()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:15:36.741482Z","iopub.execute_input":"2024-12-31T09:15:36.741742Z","iopub.status.idle":"2024-12-31T09:15:36.759683Z","shell.execute_reply.started":"2024-12-31T09:15:36.741710Z","shell.execute_reply":"2024-12-31T09:15:36.758856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_df = pl.scan_parquet(my_config.train_data_path)\ntrain_df = pl.scan_parquet(my_config.train_data_path).filter(pl.col(\"date_id\").ge(my_config.train_start_dt))\nvalid_df = pl.scan_parquet(my_config.valid_data_path)\n\ndata_stats = joblib.load('/kaggle/input/my-own-js/data_stats.pkl')\nmeans = data_stats['mean']\nstds = data_stats['std']\n\ndef standardize(df, feature_cols, means, stds):\n    return df.with_columns([\n        ((pl.col(col) - means[col]) / stds[col]).alias(col) for col in feature_cols\n    ])\n\ncategory_mappings = {'feature_09': {2: 0, 4: 1, 9: 2, 11: 3, 12: 4, 14: 5, 15: 6, 25: 7, 26: 8, 30: 9, 34: 10, 42: 11, 44: 12, 46: 13, 49: 14, 50: 15, 57: 16, 64: 17, 68: 18, 70: 19, 81: 20, 82: 21},\n 'feature_10': {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 10: 7, 12: 8},\n 'feature_11': {9: 0, 11: 1, 13: 2, 16: 3, 24: 4, 25: 5, 34: 6, 40: 7, 48: 8, 50: 9, 59: 10, 62: 11, 63: 12, 66: 13,\n  76: 14, 150: 15, 158: 16, 159: 17, 171: 18, 195: 19, 214: 20, 230: 21, 261: 22, 297: 23, 336: 24, 376: 25, 388: 26, 410: 27, 522: 28, 534: 29, 539: 30},\n}\n\ndef encode_column(df, column, mapping):\n    def encode_category(category):\n        return mapping.get(category, -1)  \n    \n    return df.with_columns(\n        pl.col(column).map_elements(encode_category, return_dtype=pl.Int16).alias(column)\n    )\n\n# 1.category encode\nfor col in my_config.feature_cat:\n    train_df = encode_column(train_df, col, category_mappings[col])\n    valid_df = encode_column(valid_df, col, category_mappings[col])\n\n# 2.standard\ntrain_df = standardize(train_df, my_config.feature_cont, means, stds)\nvalid_df = standardize(valid_df, my_config.feature_cont, means, stds)\n\n# 3.fillna 0\n\n# train_df = train_df.fill_nan(0)\n# valid_df = valid_df.fill_nan(0)\n\nprint(3)\ndf = train_df.collect().to_pandas()\ndf = reduce_mem_usage(df, False)\n\nvalid_df = valid_df.collect().to_pandas()\nvalid_df = reduce_mem_usage(valid_df, False)\nprint(3)\ndf[my_config.feature_names] = df[my_config.feature_names].fillna(0)\nvalid_df[my_config.feature_names] = valid_df[my_config.feature_names].fillna(0)\ndf = pd.concat([df, valid_df]).reset_index(drop=True)# A trick to boost LB from 0.0045->0.005","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:15:36.761772Z","iopub.execute_input":"2024-12-31T09:15:36.762023Z","iopub.status.idle":"2024-12-31T09:17:37.514548Z","shell.execute_reply.started":"2024-12-31T09:15:36.762005Z","shell.execute_reply":"2024-12-31T09:17:37.513828Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training my_configurations","metadata":{}},{"cell_type":"code","source":"import os\nimport warnings\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning import (LightningDataModule, LightningModule, Trainer)\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer\nfrom pytorch_lightning.loggers import WandbLogger\nimport wandb\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom tab_transformer_pytorch import FTTransformer\n\n\nclass custom_args():\n    def __init__(self):\n        self.usegpu = True\n        self.gpuid = 0\n        self.seed = 42\n        self.model = 'nn'\n        self.use_wandb = False\n        self.project = 'js-tabm-with-lags'\n        self.dname = \"./input_df/\"\n        self.loader_workers = 4   \n        \n        self.bs = 4096\n        self.lr = 1e-3\n        self.weight_decay = 2e-4\n        self.n_cont_features = 85\n        self.n_cat_features = 3\n        self.cat_cardinalities = [23, 10, 32]\n        self.patience = 5\n        self.max_epochs = 10\n        self.N_fold = 3\n\n        # dim = 32,                           # dimension, paper set at 32\n        # dim_out = 1,                        # binary prediction, but could be anything\n        # depth = 6,                          # depth, paper recommended 6\n        # heads = 8,                          # heads, paper recommends 8\n        # attn_dropout = 0.1,                 # post-attention dropout\n        # ff_dropout = 0.1  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:17:37.515710Z","iopub.execute_input":"2024-12-31T09:17:37.516013Z","iopub.status.idle":"2024-12-31T09:17:45.782238Z","shell.execute_reply.started":"2024-12-31T09:17:37.515983Z","shell.execute_reply":"2024-12-31T09:17:45.781285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PyTorch Data Module Definition","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, df, accelerator, num_features, cat_features, label_name,weight_name):\n        self.num_features = torch.FloatTensor(df[num_features].values).to(accelerator)\n        self.cat_features = torch.IntTensor(df[cat_features].values).to(accelerator)\n\n        self.labels = torch.FloatTensor(df[label_name].values).to(accelerator)\n        self.weights = torch.FloatTensor(df[weight_name].values).to(accelerator)\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        x_cont = self.num_features[idx]\n        x_cat = self.cat_features[idx]\n        y = self.labels[idx]\n        w = self.weights[idx]\n        return x_cont, x_cat, y, w\n\n\nclass DataModule(LightningDataModule):\n    def __init__(self, train_df, batch_size, valid_df=None, accelerator='cpu',num_features=[], cat_features=[], label_name='repsond6',weight_name='weight'):\n        super().__init__()\n        self.df = train_df\n        self.batch_size = batch_size\n        self.dates = self.df['date_id'].unique()\n        self.accelerator = accelerator\n        self.train_dataset = None\n        self.valid_df = None\n        if valid_df is not None:\n            self.valid_df = valid_df\n        self.val_dataset = None\n        self.num_features = num_features\n        self.cat_features = cat_features\n        self.label_name = label_name\n        self.weight_name = weight_name\n\n    def setup(self, fold=0, N_fold=5, stage=None):\n        # Split dataset\n        selected_dates = [date for ii, date in enumerate(self.dates) if ii % N_fold != fold]\n        df_train = self.df.loc[self.df['date_id'].isin(selected_dates)]\n        self.train_dataset = CustomDataset(df_train, self.accelerator,self.num_features,self.cat_features,self.label_name,self.weight_name)\n        if self.valid_df is not None:\n            df_valid = self.valid_df\n            self.val_dataset = CustomDataset(df_valid, self.accelerator,self.num_features,self.cat_features,self.label_name,self.weight_name)\n\n    def train_dataloader(self, n_workers=0):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=n_workers)\n\n    def val_dataloader(self, n_workers=0):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=n_workers)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:17:45.783207Z","iopub.execute_input":"2024-12-31T09:17:45.783513Z","iopub.status.idle":"2024-12-31T09:17:45.793054Z","shell.execute_reply.started":"2024-12-31T09:17:45.783486Z","shell.execute_reply":"2024-12-31T09:17:45.792295Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NN Model Definition¶","metadata":{}},{"cell_type":"code","source":"class R2Loss(nn.Module):\n    def __init__(self):\n        super(R2Loss, self).__init__()\n\n    def forward(self, y_pred, y_true):\n        mse_loss = torch.sum((y_pred - y_true) ** 2)\n        var_y = torch.sum(y_true ** 2)\n        loss = mse_loss / (var_y + 1e-38)\n        return loss\n\n# Custom R2 metric for validation\ndef r2_val(y_true, y_pred, sample_weight):\n    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n    return r2\n\n\nclass FTTransformerModel(LightningModule):\n    def __init__(self, n_cont_features, cat_cardinalities, lr, weight_decay):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = FTTransformer(\n                categories = cat_cardinalities,      # tuple containing the number of unique values within each category\n                num_continuous = n_cont_features,                # number of continuous values\n                dim = 8,                           # dimension, paper set at 32\n                dim_out = 1,                        # binary prediction, but could be anything\n                depth = 3,                          # depth, paper recommended 6\n                heads = 2,                          # heads, paper recommends 8\n                attn_dropout = 0.2,                 # post-attention dropout\n                ff_dropout = 0.2                    # feed forward dropout\n            )\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.training_step_outputs = []\n        self.validation_step_outputs = []\n        # self.loss_fn = F.mse_loss()\n        # self.loss_fn = R2Loss()\n        # self.loss_fn = weighted_mse_loss\n\n    def forward(self, x_cont, x_cat):\n        return self.model(x_cat, x_cont).squeeze(-1)\n        # return self.model(x_cont, x_cat).squeeze(-1)\n\n    def training_step(self, batch):\n        x_cont, x_cat, y, w = batch\n        # x_cont = x_cont + torch.randn_like(x_cont) * 0.01\n        y_hat = self(x_cont, x_cat)\n        # loss = self.loss_fn(y_hat.flatten(0, 1), y.repeat_interleave(self.k), w_y.repeat_interleave(self.k))\n        # loss = self.loss_fn(y_hat.flatten(0, 1), y.repeat_interleave(self.k))\n        \n        loss = F.mse_loss(y_hat, y, reduction='none') * w  #\n        loss = loss.mean()\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=x_cont.size(0))\n        self.training_step_outputs.append((y_hat , y, w))\n        return loss\n\n    def validation_step(self, batch):\n        x_cont, x_cat, y, w = batch\n        # x_cont = x_cont + torch.randn_like(x_cont)\n        y_hat = self(x_cont, x_cat)\n        # loss = self.loss_fn(y_hat.flatten(0, 1), y.repeat_interleave(self.k), w_y.repeat_interleave(self.k))\n        # loss = self.loss_fn(y_hat.flatten(0, 1), y.repeat_interleave(self.k))\n        loss = F.mse_loss(y_hat, y, reduction='none') * w  #\n        loss = loss.mean()\n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=x_cont.size(0))\n        self.validation_step_outputs.append((y_hat , y, w))\n        return loss\n\n    def on_validation_epoch_end(self):\n        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n        if self.trainer.sanity_checking:\n            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n        else:\n            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n            # r2_val\n            val_r_square = r2_val(y, prob, weights)\n            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n        self.validation_step_outputs.clear()\n\n    # def my_configure_optimizers(self):\n    #     optimizer = torch.optim.AdamW(make_parameter_groups(self.model), lr=self.lr, weight_decay=self.weight_decay)\n    #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5,\n    #                                                            verbose=True)\n    #     return {\n    #         'optimizer': optimizer,\n    #         'lr_scheduler': {\n    #             'scheduler': scheduler,\n    #             'monitor': 'val_r_square',\n    #         }\n    #     }\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n                                                               verbose=True)\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'monitor': 'val_loss',\n            }\n        }\n\n    def on_train_epoch_end(self):\n        if self.trainer.sanity_checking:\n            return\n\n        y = torch.cat([x[1] for x in self.training_step_outputs]).cpu().numpy()\n        prob = torch.cat([x[0] for x in self.training_step_outputs]).detach().cpu().numpy()\n        weights = torch.cat([x[2] for x in self.training_step_outputs]).cpu().numpy()\n        # r2_training\n        train_r_square = r2_val(y, prob, weights)\n        self.log(\"train_r_square\", train_r_square, prog_bar=True, on_step=False, on_epoch=True)\n        self.training_step_outputs.clear()\n\n        epoch = self.trainer.current_epoch\n        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n        print(f\"Epoch {epoch}: {formatted_metrics}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:17:45.794128Z","iopub.execute_input":"2024-12-31T09:17:45.794458Z","iopub.status.idle":"2024-12-31T09:17:45.826628Z","shell.execute_reply.started":"2024-12-31T09:17:45.794428Z","shell.execute_reply":"2024-12-31T09:17:45.825732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"args = custom_args()\n# checking device\ndevice = torch.device(f'cuda:{args.gpuid}' if torch.cuda.is_available() and args.usegpu else 'cpu')\naccelerator = 'gpu' if torch.cuda.is_available() and args.usegpu else 'cpu'\nloader_device = 'cpu'\n\n\n# Initialize Data Module\ndata_module = DataModule(df, batch_size=args.bs, valid_df=valid_df, accelerator=loader_device,num_features=my_config.feature_cont, cat_features=my_config.feature_cat, label_name=my_config.label_name,weight_name=my_config.weight_name)\n\ndel df\ndel valid_df\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:17:45.827359Z","iopub.execute_input":"2024-12-31T09:17:45.827642Z","iopub.status.idle":"2024-12-31T09:17:46.136103Z","shell.execute_reply.started":"2024-12-31T09:17:45.827620Z","shell.execute_reply":"2024-12-31T09:17:46.135384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\npl.seed_everything(args.seed)\nfor fold in range(args.N_fold):\n    data_module.setup(fold, args.N_fold)\n    # Obtain input dimension\n    # input_dim = data_module.train_dataset.features.shape[1]\n    # Initialize Model\n    model = FTTransformerModel(\n        n_cont_features = args.n_cont_features, \n        cat_cardinalities = args.cat_cardinalities,\n        lr=args.lr,\n        weight_decay=args.weight_decay\n    )\n    # Initialize Logger\n    if args.use_wandb:\n        wandb_run = wandb.init(project=args.project, my_config=vars(args), reinit=True)\n        logger = WandbLogger(experiment=wandb_run)\n    else:\n        logger = None\n    # Initialize Callbacks\n    early_stopping = EarlyStopping('val_r_square', patience=args.patience, mode='max', verbose=False)\n    checkpoint_callback = ModelCheckpoint(monitor='val_r_square', mode='max', save_top_k=1, verbose=False, filename=f\"./models/nn_{fold}.model\") \n    timer = Timer()\n    # Initialize Trainer\n    trainer = Trainer(\n        max_epochs=args.max_epochs,\n        accelerator=accelerator,\n        devices=[args.gpuid] if args.usegpu else None,\n        logger=logger,\n        callbacks=[early_stopping, checkpoint_callback, timer],\n        enable_progress_bar=True\n    )\n    # Start Training\n    trainer.fit(model, data_module.train_dataloader(args.loader_workers), data_module.val_dataloader(args.loader_workers))\n    # You can find trained best model in your local path\n    print(f'Fold-{fold} Training completed in {timer.time_elapsed(\"train\"):.2f}s')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:17:46.136789Z","iopub.execute_input":"2024-12-31T09:17:46.137039Z","execution_failed":"2024-12-31T09:20:43.276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}