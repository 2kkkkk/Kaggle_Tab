{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":203900450,"sourceType":"kernelVersion"},{"sourceId":215383590,"sourceType":"kernelVersion"}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pickle\nimport polars as pl\nimport numpy as np\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T13:27:50.708264Z","iopub.execute_input":"2025-01-03T13:27:50.708604Z","iopub.status.idle":"2025-01-03T13:27:51.219561Z","shell.execute_reply.started":"2025-01-03T13:27:50.708555Z","shell.execute_reply":"2025-01-03T13:27:51.218899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reduce_mem_usage(df, float16_as32=True):\n    #memory_usage()是df每列的内存使用量,sum是对它们求和, B->KB->MB\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:#遍历每列的列名\n        col_type = df[col].dtype#列名的type\n        if col_type != object and str(col_type)!='category':#不是object也就是说这里处理的是数值类型的变量\n            c_min,c_max = df[col].min(),df[col].max() #求出这列的最大值和最小值\n            if str(col_type)[:3] == 'int':#如果是int类型的变量,不管是int8,int16,int32还是int64\n                #如果这列的取值范围是在int8的取值范围内,那就对类型进行转换 (-128 到 127)\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                #如果这列的取值范围是在int16的取值范围内,那就对类型进行转换(-32,768 到 32,767)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                #如果这列的取值范围是在int32的取值范围内,那就对类型进行转换(-2,147,483,648到2,147,483,647)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                #如果这列的取值范围是在int64的取值范围内,那就对类型进行转换(-9,223,372,036,854,775,808到9,223,372,036,854,775,807)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:#如果是浮点数类型.\n                #如果数值在float16的取值范围内,如果觉得需要更高精度可以考虑float32\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    if float16_as32:#如果数据需要更高的精度可以选择float32\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float16)  \n                #如果数值在float32的取值范围内，对它进行类型转换\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                #如果数值在float64的取值范围内，对它进行类型转换\n                else:\n                    df[col] = df[col].astype(np.float64)\n    #计算一下结束后的内存\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #相比一开始的内存减少了百分之多少\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T13:27:51.220249Z","iopub.execute_input":"2025-01-03T13:27:51.220606Z","iopub.status.idle":"2025-01-03T13:27:51.229062Z","shell.execute_reply.started":"2025-01-03T13:27:51.220583Z","shell.execute_reply":"2025-01-03T13:27:51.228149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_names = [f\"feature_{i:02d}\" for i in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\nfeature_cat = [\"feature_09\", \"feature_10\", \"feature_11\"]\nfeature_cont = [item for item in feature_names if item not in feature_cat]\nlabel_name = 'responder_6'\nweight_name = 'weight'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T13:27:51.229745Z","iopub.execute_input":"2025-01-03T13:27:51.229927Z","iopub.status.idle":"2025-01-03T13:27:51.248148Z","shell.execute_reply.started":"2025-01-03T13:27:51.229911Z","shell.execute_reply":"2025-01-03T13:27:51.247547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data_path = '/kaggle/input/data-create-create-lags/training.parquet'\nvalid_data_path = '/kaggle/input/data-create-create-lags/validation.parquet'\n\ntrain_start_dt = 1100\nvalid_start_dt = 1638 # last 60 days #1670\n\ntrain_df = pl.scan_parquet(train_data_path)\nvalid_df = pl.scan_parquet(valid_data_path)\ndf = pl.concat([train_df,valid_df])\n\ntrain_df = df.filter(pl.col(\"date_id\").gt(train_start_dt)) # .lt(valid_start_dt)\nvalid_df = df.filter(pl.col(\"date_id\").ge(valid_start_dt))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T13:27:51.249914Z","iopub.execute_input":"2025-01-03T13:27:51.250135Z","iopub.status.idle":"2025-01-03T13:27:51.284982Z","shell.execute_reply.started":"2025-01-03T13:27:51.250116Z","shell.execute_reply":"2025-01-03T13:27:51.284323Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# pytorch","metadata":{}},{"cell_type":"code","source":"import os\nimport warnings\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning import (LightningDataModule, LightningModule, Trainer)\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer\nfrom pytorch_lightning.loggers import WandbLogger\nimport wandb\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T13:27:51.285939Z","iopub.execute_input":"2025-01-03T13:27:51.286181Z","iopub.status.idle":"2025-01-03T13:28:00.161716Z","shell.execute_reply.started":"2025-01-03T13:27:51.286162Z","shell.execute_reply":"2025-01-03T13:28:00.160802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, df, accelerator):\n        self.cont_features = torch.FloatTensor(df[feature_cont].values).to(accelerator)\n        self.cat_features = torch.IntTensor(df[feature_cat].values).to(accelerator)\n\n        self.labels = torch.FloatTensor(df[label_name].values).to(accelerator)\n        self.weights = torch.FloatTensor(df[weight_name].values).to(accelerator)\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        x_cont = self.cont_features[idx]\n        x_cat = self.cat_features[idx]\n        y = self.labels[idx]\n        w = self.weights[idx]\n        return x_cont, x_cat, y, w\n\n\nclass DataModule(LightningDataModule):\n    def __init__(self, train_df, batch_size, valid_df=None, accelerator='cpu'):\n        super().__init__()\n        self.df = train_df\n        self.batch_size = batch_size\n        self.dates = train_df['date_id'].unique()\n        self.accelerator = accelerator\n        self.train_dataset = None\n        self.valid_df = None\n        if valid_df is not None:\n            self.valid_df = valid_df\n        self.val_dataset = None\n        \n    def setup(self, fold=0, N_fold=5, stage=None):\n       # Split dataset\n       if N_fold == 1:\n           df_train = self.df\n       else:\n           selected_dates = [date for ii, date in enumerate(self.dates) if ii % N_fold != fold]\n           df_train = self.df.loc[self.df['date_id'].isin(selected_dates)]\n       \n       self.train_dataset = CustomDataset(df_train, self.accelerator)\n       if self.valid_df is not None:\n           df_valid = self.valid_df\n           self.val_dataset = CustomDataset(df_valid, self.accelerator)\n\n    def train_dataloader(self, n_workers=0):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=n_workers)\n\n    def val_dataloader(self, n_workers=0):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=n_workers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T13:28:00.162637Z","iopub.execute_input":"2025-01-03T13:28:00.162879Z","iopub.status.idle":"2025-01-03T13:28:00.171631Z","shell.execute_reply.started":"2025-01-03T13:28:00.162857Z","shell.execute_reply":"2025-01-03T13:28:00.170699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom R2 metric for validation\ndef r2_val(y_true, y_pred, sample_weight):\n    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n    return r2\n\n\nclass NN(LightningModule):\n    def __init__(self, emb_dims, cat_dropout, cont_dim_original, cont_dim, cont_dropout, hidden_dims, dropouts, lr, weight_decay):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # cat\n        self.emb = nn.ModuleList([nn.Embedding(x,y) for x,y in emb_dims])\n        no_of_embs = sum([y for x, y in emb_dims])\n        self.no_of_embs = no_of_embs\n        self.cat_dropout = nn.Dropout(cat_dropout)\n        \n        # cont \n        self.cont_batchnorm = nn.BatchNorm1d(cont_dim_original)\n        self.cont_dropout = nn.Dropout(cont_dropout)\n        self.cont_dense = nn.Linear(cont_dim_original, cont_dim)\n        self.cont_activation = nn.ReLU()\n        \n        # concat  model \n        layers = []\n        in_dim = cont_dim + no_of_embs\n        for i, hidden_dim in enumerate(hidden_dims):\n            layers.append(nn.BatchNorm1d(in_dim))\n            if i > 0:\n                layers.append(nn.SiLU())\n            if i < len(dropouts):\n                layers.append(nn.Dropout(dropouts[i]))\n            layers.append(nn.Linear(in_dim, hidden_dim))\n            # layers.append(nn.ReLU())\n            in_dim = hidden_dim\n        layers.append(nn.Linear(in_dim, 1)) \n        layers.append(nn.Tanh())\n        self.model = nn.Sequential(*layers)\n        \n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.validation_step_outputs = []\n\n    def forward(self, x_cont, x_cat):\n        ##cont data\n        x_cont = self.cont_batchnorm(x_cont)\n        x_cont = self.cont_dropout(x_cont)\n        x_cont = self.cont_dense(x_cont)\n        x_cont = self.cont_activation(x_cont)\n        \n        ## cat data part\n        x_cat = [emb_layer(x_cat[:,i]) for i, emb_layer in enumerate(self.emb)]\n        x_cat = torch.cat(x_cat,1)\n        x_cat = self.cat_dropout(x_cat)\n        \n        ##concat\n        x = torch.cat([x_cont,x_cat],1)\n        \n        return 5 * self.model(x).squeeze(-1)  \n\n    def training_step(self, batch):\n        x_cont,x_cat, y, w = batch\n        y_hat = self(x_cont,x_cat)\n        loss = F.mse_loss(y_hat, y, reduction='none') * w  #\n        loss = loss.mean()\n        self.log('train_loss', loss, on_step=False, on_epoch=True, batch_size=x_cont.size(0))\n        return loss\n\n    def validation_step(self, batch):\n        x_cont,x_cat, y, w = batch\n        y_hat = self(x_cont,x_cat)\n        loss = F.mse_loss(y_hat, y, reduction='none') * w\n        loss = loss.mean()\n        self.log('val_loss', loss, on_step=False, on_epoch=True, batch_size=x_cont.size(0))\n        self.validation_step_outputs.append((y_hat, y, w))\n        return loss\n\n    def on_validation_epoch_end(self):\n        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n        if self.trainer.sanity_checking:\n            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n        else:\n            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n            # r2_val\n            val_r_square = r2_val(y, prob, weights)\n            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n        self.validation_step_outputs.clear()\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n                                                               verbose=True)\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'monitor': 'val_loss',\n            }\n        }\n\n    def on_train_epoch_end(self):\n        if self.trainer.sanity_checking:\n            return\n        epoch = self.trainer.current_epoch\n        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n        print(f\"Epoch {epoch}: {formatted_metrics}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T13:28:00.172595Z","iopub.execute_input":"2025-01-03T13:28:00.172885Z","iopub.status.idle":"2025-01-03T13:28:00.203167Z","shell.execute_reply.started":"2025-01-03T13:28:00.172854Z","shell.execute_reply":"2025-01-03T13:28:00.202560Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create PyTorch Data Module","metadata":{}},{"cell_type":"code","source":"import polars as pl\n\ncategory_mappings = {'feature_09': {2: 0, 4: 1, 9: 2, 11: 3, 12: 4, 14: 5, 15: 6, 25: 7, 26: 8, 30: 9, 34: 10, 42: 11, 44: 12, 46: 13, 49: 14, 50: 15, 57: 16, 64: 17, 68: 18, 70: 19, 81: 20, 82: 21},\n 'feature_10': {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 10: 7, 12: 8},\n 'feature_11': {9: 0, 11: 1, 13: 2, 16: 3, 24: 4, 25: 5, 34: 6, 40: 7, 48: 8, 50: 9, 59: 10, 62: 11, 63: 12, 66: 13,\n  76: 14, 150: 15, 158: 16, 159: 17, 171: 18, 195: 19, 214: 20, 230: 21, 261: 22, 297: 23, 336: 24, 376: 25, 388: 26, 410: 27, 522: 28, 534: 29, 539: 30},\n}\n\ndef encode_column(df, column, mapping):\n    def encode_category(category):\n        return mapping.get(category, -1)  \n    \n    return df.with_columns(\n        pl.col(column).map_elements(encode_category, return_dtype=pl.Int16).alias(column)\n    )\n\n# 1.category encode\nfor col in feature_cat:\n    train_df = encode_column(train_df, col, category_mappings[col])\n    valid_df = encode_column(valid_df, col, category_mappings[col])\n\ntrain_df = train_df.collect().to_pandas()\nvalid_df = valid_df.collect().to_pandas()\n\ntrain_df = reduce_mem_usage(train_df, False)\nvalid_df = reduce_mem_usage(valid_df, False)\n\n# train_df = pd.concat([train_df,valid_df],axis=0).reset_index(drop=True)\nprint(train_df.shape,valid_df.shape)\n\ntrain_df[feature_names] = train_df[feature_names].fillna(0)\nvalid_df[feature_names] = valid_df[feature_names].fillna(0)\n\nimport pytorch_lightning as pl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T13:28:00.203948Z","iopub.execute_input":"2025-01-03T13:28:00.204153Z","iopub.status.idle":"2025-01-03T13:29:45.621552Z","shell.execute_reply.started":"2025-01-03T13:28:00.204136Z","shell.execute_reply":"2025-01-03T13:29:45.620842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class custom_args():\n    def __init__(self):\n        self.usegpu = True\n        self.gpuid = 0\n        self.seed = 42\n        self.model = 'nn'\n        self.use_wandb = False\n        self.project = 'js-xs-nn-with-lags'\n        self.dname = \"./input_df/\"\n        self.loader_workers = 4\n        self.bs = 8192\n        self.lr = 1e-3\n        self.weight_decay = 5e-4\n        self.dropouts = [0.1, 0.1]\n        self.n_hidden = [256, 256, 128]\n        self.patience = 5\n        self.max_epochs = 50\n        self.N_fold = 1\n\n        # cat\n        self.n_cont_features = 85\n        # self.n_cat_features = 3\n        cat_cardinalities = [23, 10, 32]\n        self.emb_dims = [(dim, min(50,(dim+1)//3)) for dim in cat_cardinalities]\n        self.cat_dropout = 0.1\n\n        # cont\n        self.cont_dim = 512\n        self.cont_dropout = 0.1\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T13:29:45.622258Z","iopub.execute_input":"2025-01-03T13:29:45.622541Z","iopub.status.idle":"2025-01-03T13:29:45.628314Z","shell.execute_reply.started":"2025-01-03T13:29:45.622518Z","shell.execute_reply":"2025-01-03T13:29:45.627331Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create Model and Training","metadata":{}},{"cell_type":"code","source":"args = custom_args()\n\n# checking device\ndevice = torch.device(f'cuda:{args.gpuid}' if torch.cuda.is_available() and args.usegpu else 'cpu')\naccelerator = 'gpu' if torch.cuda.is_available() and args.usegpu else 'cpu'\nloader_device = 'cpu'\n\ndata_module = DataModule(train_df, batch_size=args.bs, valid_df=valid_df, accelerator=loader_device)\n\nimport gc\ndel train_df,valid_df\ngc.collect()\n\n\npl.seed_everything(args.seed)\n\n# Obtain input dimension\n# input_dim = data_module.train_dataset.features.shape[1]\n# Initialize Model\nfor fold in range(args.N_fold):\n    data_module.setup(fold, args.N_fold)\n    model = NN(\n        emb_dims = args.emb_dims, \n        cat_dropout = args.cat_dropout,\n        cont_dim_original = args.n_cont_features,\n        cont_dim = args.cont_dim,\n        cont_dropout = args.cont_dropout,\n        hidden_dims=args.n_hidden,\n        dropouts=args.dropouts,\n        lr=args.lr,\n        weight_decay=args.weight_decay\n    )\n    # Initialize Logger\n    if args.use_wandb:\n        wandb_run = wandb.init(project=args.project, config=vars(args), reinit=True)\n        logger = WandbLogger(experiment=wandb_run)\n    else:\n        logger = None\n    # Initialize Callbacks\n    # early_stopping = EarlyStopping('val_loss', patience=args.patience, mode='min', verbose=False)\n    # checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1, verbose=False, filename=f\"./models/nn_train_{train_start_dt}_valid_{valid_start_dt}.model\") \n    \n    early_stopping = EarlyStopping('val_r_square', patience=args.patience, mode='max', verbose=False)\n    checkpoint_callback = ModelCheckpoint(monitor='val_r_square', mode='max', save_top_k=1, verbose=False, filename=f\"./models/nn_fold{fold}_train{train_start_dt}_valid{valid_start_dt}.model\") \n    \n    timer = Timer()\n    # Initialize Trainer\n    trainer = Trainer(\n        max_epochs=args.max_epochs,\n        accelerator=accelerator,\n        devices=[args.gpuid] if args.usegpu else None,\n        logger=logger,\n        callbacks=[early_stopping, checkpoint_callback, timer],\n        enable_progress_bar=True\n    )\n    # Start Training\n    trainer.fit(model, data_module.train_dataloader(args.loader_workers), data_module.val_dataloader(args.loader_workers))\n    # You can find trained best model in your local path\n    print(f'Fold-{fold} Training completed in {timer.time_elapsed(\"train\"):.2f}s')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T13:29:45.629036Z","iopub.execute_input":"2025-01-03T13:29:45.629211Z","execution_failed":"2025-01-03T13:30:57.105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}