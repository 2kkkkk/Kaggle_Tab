{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":10377588,"sourceType":"datasetVersion","datasetId":6428330}],"dockerImageVersionId":30823,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport polars as pl\nimport numpy as np\nimport os, gc\nimport time\nfrom tqdm.auto import tqdm\nfrom matplotlib import pyplot as plt\nimport pickle\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pytorch_lightning import (LightningDataModule, LightningModule, Trainer)\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\n\n\nfrom sklearn.metrics import r2_score\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nimport kaggle_evaluation.jane_street_inference_server","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:38:49.660540Z","iopub.execute_input":"2025-01-11T10:38:49.660886Z","iopub.status.idle":"2025-01-11T10:38:49.666857Z","shell.execute_reply.started":"2025-01-11T10:38:49.660855Z","shell.execute_reply":"2025-01-11T10:38:49.665909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NN(LightningModule):\n    def __init__(self, input_dim, hidden_dims, dropouts, lr, weight_decay):\n        super().__init__()\n        self.save_hyperparameters()\n        layers = []\n        in_dim = input_dim\n        for i, hidden_dim in enumerate(hidden_dims):\n            layers.append(nn.BatchNorm1d(in_dim))\n            if i > 0:\n                layers.append(nn.SiLU())\n            if i < len(dropouts):\n                layers.append(nn.Dropout(dropouts[i]))\n            layers.append(nn.Linear(in_dim, hidden_dim))\n            # layers.append(nn.ReLU())\n            in_dim = hidden_dim\n        layers.append(nn.Linear(in_dim, 1)) \n        layers.append(nn.Tanh())\n        self.model = nn.Sequential(*layers)\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.validation_step_outputs = []\n\n    def forward(self, x):\n        return 5 * self.model(x).squeeze(-1)  \n\n    def training_step(self, batch):\n        x, y, w = batch\n        y_hat = self(x)\n        loss = F.mse_loss(y_hat, y, reduction='none') * w  #\n        loss = loss.mean()\n        self.log('train_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n        return loss\n\n    def validation_step(self, batch):\n        x, y, w = batch\n        y_hat = self(x)\n        loss = F.mse_loss(y_hat, y, reduction='none') * w\n        loss = loss.mean()\n        self.log('val_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n        self.validation_step_outputs.append((y_hat, y, w))\n        return loss\n\n    def on_validation_epoch_end(self):\n        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n        if self.trainer.sanity_checking:\n            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n        else:\n            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n            # r2_val\n            val_r_square = r2_val(y, prob, weights)\n            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n        self.validation_step_outputs.clear()\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n                                                               verbose=True)\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'monitor': 'val_loss',\n            }\n        }\n\n    def on_train_epoch_end(self):\n        if self.trainer.sanity_checking:\n            return\n        epoch = self.trainer.current_epoch\n        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n        print(f\"Epoch {epoch}: {formatted_metrics}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:38:49.668001Z","iopub.execute_input":"2025-01-11T10:38:49.668265Z","iopub.status.idle":"2025-01-11T10:38:49.683111Z","shell.execute_reply.started":"2025-01-11T10:38:49.668245Z","shell.execute_reply":"2025-01-11T10:38:49.682320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, X_data, Y_data):\n        \"\"\"\n        初始化数据集，X_data 和 Y_data 是两个列表或数组\n        X_data: 输入特征\n        Y_data: 目标标签\n        \"\"\"\n        self.X_data = X_data\n        self.Y_data = Y_data\n\n    def __len__(self):\n        \"\"\"返回数据集的大小\"\"\"\n        return len(self.X_data)\n\n    def __getitem__(self, idx):\n        \"\"\"返回指定索引的数据\"\"\"\n        x = torch.tensor(self.X_data[idx], dtype=torch.float32)  # 转换为 Tensor\n        y = torch.tensor(self.Y_data[idx], dtype=torch.float32)\n        return x, y\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:38:49.684596Z","iopub.execute_input":"2025-01-11T10:38:49.684861Z","iopub.status.idle":"2025-01-11T10:38:49.698560Z","shell.execute_reply.started":"2025-01-11T10:38:49.684841Z","shell.execute_reply":"2025-01-11T10:38:49.697895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_names = [f\"feature_{i:02d}\" for i in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\nfeature_names = [item for item in feature_names if item not in ['feature_09','feature_10','feature_11']]\n\ntarget_name = 'responder_6'\nweight_name = 'weight'\n\ndevice = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n\ncheckpoint_path = '/kaggle/input/no-feature09-10-11/nn_train_1100_valid_1638.model (4).ckpt'\n\n\nDEBUG = False\n\nretrain_every_n_days = 30\nretrain_after_n_days = 30\nretrain_last_n_days_data = 30\n\nretrain_epochs = 1\nretrain_bs = 8196\nretrain_lr = 1e-4\nretrain_wd = retrain_lr/5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:38:49.699358Z","iopub.execute_input":"2025-01-11T10:38:49.699615Z","iopub.status.idle":"2025-01-11T10:38:49.712508Z","shell.execute_reply.started":"2025-01-11T10:38:49.699584Z","shell.execute_reply":"2025-01-11T10:38:49.711591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize global vars\ncache = None\ncache_list = []\nday_count = 0\nmodel = NN.load_from_checkpoint(checkpoint_path).to(device)\nlags_ : pl.DataFrame | None = None\nlabels : pl.DataFrame | None = None\nlags_last : pl.DataFrame | None = None\n\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=retrain_lr, weight_decay=retrain_wd)\n# optimizer = torch.optim.Adam(model.parameters(), lr=retrain_lr)\n\ndef predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n    global cache          # Declare the global cache\n    global cache_list\n    global day_count\n    global model\n    global lags_\n    global labels\n    global lags_last\n    global loss_fn\n    global optimizer\n    \n    id_column_types = {\n        'date_id': pl.Int16,\n        'time_id': pl.Int16,\n        'symbol_id': pl.Int16\n    }\n    test = test.cast(id_column_types)\n    \n    if lags is not None:\n        lags_ = lags\n        lags = lags.cast(id_column_types)\n        lags_last = lags.group_by([\"date_id\", \"symbol_id\"], maintain_order=True).last() # pick up last record of previous date\n        day_count += 1\n        # store ground truth from previous day\n        update_labels = lags[\"date_id\", \"symbol_id\", \"time_id\",\"responder_6_lag_1\"]\n        update_labels = update_labels.rename({\"responder_6_lag_1\": \"responder_6\"})\n        if labels is not None:\n            labels = pl.concat([labels, update_labels], rechunk=True)\n        else:\n            labels = update_labels\n    \n    # join lag feature\n    test = test.join(lags_last, on=[\"date_id\", \"symbol_id\"],  how=\"left\")\n    \n    # store data for each batch\n    cache_list.append(test)\n\n    # ======================== retrain part ====================================================\n    # re-train a model on the fly every N days\n    if lags is not None and day_count >= retrain_after_n_days and day_count % retrain_every_n_days == 0 :\n        # 更新cache 1.合并旧cache和cache_update 2.保留cache最新的time_window天\n        if cache is not None:\n            cache_update = pl.concat(cache_list, rechunk=True)\n            cache = pl.concat([cache, cache_update], rechunk=True)\n        else:\n            cache = pl.concat(cache_list, rechunk=True)\n        \n        # store only last time_window days\n        days = np.sort(np.unique(cache[\"date_id\"].to_numpy()))\n        days = days[-retrain_last_n_days_data:]\n        min_day = np.min(days) - 1 # 因为cache_list已经append今天time_id=0时的test了，所以cache中的最大date_id是今天,如果用一天的数据训练,应该-1\n        cache = cache.filter(pl.col(\"date_id\") >= min_day)\n        print('cache最小最大date_id',np.min(cache[\"date_id\"].to_numpy()),np.max(cache[\"date_id\"].to_numpy()))\n        # filter labels\n        # move data back to the previous day (we receive the lags at the same day but they are the ground truth of the previous day)\n        labels_ = labels.with_columns(\n            (pl.col(\"date_id\") -1).alias(\"date_id\")\n        )\n        labels_ = labels_.filter(pl.col(\"date_id\") >= np.min(cache[\"date_id\"].to_numpy()))\n        \n        # prepare data for training\n        train_df = cache.join(labels_, on=[\"date_id\", \"symbol_id\", \"time_id\"],  how=\"left\")\n        X_train = train_df[feature_names].to_numpy()\n        y_train = train_df[target_name].to_numpy().flatten()\n        X_train = np.nan_to_num(X_train, nan=0.0)\n        y_train = np.nan_to_num(y_train, nan=0.0)\n        dataset = MyDataset(X_train, y_train)\n        dataloader = DataLoader(dataset, batch_size=retrain_bs, shuffle=True)\n        \n        \n        # Re-train the model\n        for epoch in range(retrain_epochs):\n            model.train()\n            for batch, (X, y) in enumerate(dataloader):\n                pred = model(X.to(device))\n                loss = loss_fn(pred, y.to(device))\n                # Backpropagation\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                \n        # reset counter otherwise we will retrain for each time_id of the same day\n        # empty cache list\n        cache_list = []\n    \n    # fill na 0\n    test_input = test[feature_names].to_pandas()\n    test_input = test_input.fillna(0)\n\n    preds = np.zeros((test.shape[0],))\n    test_input = torch.FloatTensor(test_input.values).to(device)\n    with torch.no_grad():\n        model.eval()\n        preds += model(test_input).cpu().numpy()\n    # print(f\"predict> preds.shape =\", preds.shape)\n    \n    predictions = \\\n    test.select('row_id').\\\n    with_columns(\n        pl.Series(\n            name   = 'responder_6', \n            values = np.clip(preds, a_min = -5, a_max = 5),\n            dtype  = pl.Float64,\n        )\n    )\n\n    # The predict function must return a DataFrame\n    assert isinstance(predictions, pl.DataFrame | pd.DataFrame)\n    # with columns 'row_id', 'responer_6'\n    assert list(predictions.columns) == ['row_id', 'responder_6']\n    # and as many rows as the test data.\n    assert len(predictions) == len(test)\n\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:38:49.795342Z","iopub.execute_input":"2025-01-11T10:38:49.795584Z","iopub.status.idle":"2025-01-11T10:38:49.898545Z","shell.execute_reply.started":"2025-01-11T10:38:49.795565Z","shell.execute_reply":"2025-01-11T10:38:49.897654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n        )\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:38:49.899703Z","iopub.execute_input":"2025-01-11T10:38:49.899983Z","iopub.status.idle":"2025-01-11T10:38:49.903605Z","shell.execute_reply.started":"2025-01-11T10:38:49.899963Z","shell.execute_reply":"2025-01-11T10:38:49.902774Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# debug","metadata":{}},{"cell_type":"code","source":"def makelag(date_id):\n    \"\"\"\n    Making lag at the previout day\n\n    Args:\n    date_id (int): date_id at the previout day\n    \n    Returns:\n    pl.dataframe\n    \"\"\"\n    responder_cols = [s for s in train.columns if \"responder\" in s]\n    lag = alltraindata.filter(pl.col(\"date_id\")==date_id).select([\"date_id\",\"time_id\",\"symbol_id\"] + responder_cols).collect()\n    lag.columns = lag_sample.columns\n    \n    return lag\n\ndef weighted_zero_mean_r2(y_true, y_pred, weights):\n    \"\"\"\n    Calculate the sample weighted zero-mean R-squared score.\n\n    Parameters:\n    y_true (numpy.ndarray): Ground-truth values for responder_6.\n    y_pred (numpy.ndarray): Predicted values for responder_6.\n    weights (numpy.ndarray): Sample weight vector.\n\n    Returns:\n    float: The weighted zero-mean R-squared score.\n    \"\"\"\n    numerator = np.sum(weights * (y_true - y_pred)**2)\n    denominator = np.sum(weights * y_true**2)\n    \n    r2_score = 1 - numerator / denominator\n    return r2_score\n\n\nif DEBUG:\n    all_submission_dataframe = []\n    lag_sample = pl.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet/date_id=0/part-0.parquet\")\n    alltraindata = pl.scan_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet\")\n    # pick 50 days\n    nb_days = 60\n    train = alltraindata.filter(pl.col(\"date_id\")>1698-nb_days).collect()\n    train = train.with_columns(pl.Series(range(len(train))).alias(\"row_id\"))\n\n    ## Step 1 The data is split by day using group_by.\n    for num_days, df_per_day in train.group_by(\"date_id\",maintain_order=True):\n        \n        ## Step 2 The data is split by time_id using group_by, and the lag is generated (for time_id == 0).\n        \n        for time_id, test in df_per_day.group_by(\"time_id\",maintain_order=True):\n            \n            ## when time_id == 0, makelags\n            \n            if time_id[0] == 0:\n                lag = makelag(num_days[0] - 1)\n            else:\n                lag = None\n            start_time = time.time()\n            submission_dataframe = predict(test, lag)\n            elapsed = time.time() - start_time\n            if elapsed>30:\n                print(num_days,time_id)\n                print('耗时：{}'.format(elapsed))\n            all_submission_dataframe.append(submission_dataframe)\n            \n    all_submission_dataframe = pl.concat(all_submission_dataframe)\n    \n\n    print(weighted_zero_mean_r2(train.select(\"responder_6\").to_numpy().reshape(-1), all_submission_dataframe.select(\"responder_6\").to_numpy().reshape(-1), train.select(\"weight\").to_numpy().reshape(-1)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:38:49.905025Z","iopub.execute_input":"2025-01-11T10:38:49.905235Z","iopub.status.idle":"2025-01-11T10:43:11.557238Z","shell.execute_reply.started":"2025-01-11T10:38:49.905216Z","shell.execute_reply":"2025-01-11T10:43:11.556266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:43:11.558159Z","iopub.execute_input":"2025-01-11T10:43:11.558405Z","iopub.status.idle":"2025-01-11T10:43:11.563379Z","shell.execute_reply.started":"2025-01-11T10:43:11.558384Z","shell.execute_reply":"2025-01-11T10:43:11.562477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
