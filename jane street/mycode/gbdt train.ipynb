{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":203900450,"sourceType":"kernelVersion"},{"sourceId":216602411,"sourceType":"kernelVersion"}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os,gc,pickle\nimport joblib \nfrom tqdm import tqdm\nimport pandas as pd\nimport polars as pl\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cbt\nimport numpy as np \nfrom sklearn.metrics import r2_score\n\nfrom joblib import Parallel, delayed\n\nimport kaggle_evaluation.jane_street_inference_server\n\n# !pip install lightgbm==4.2.0 -i https://mirrors.aliyun.com/pypi/simple/\n# !pip install catboost==1.2.7 -i https://mirrors.aliyun.com/pypi/simple/\n# !pip install xgboost==2.0.3 -i https://mirrors.aliyun.com/pypi/simple/\n# !pip install joblib==1.4.2 -i https://mirrors.aliyun.com/pypi/simple/\n\n\ndef reduce_mem_usage(df, float16_as32=True):\n    #memory_usage()是df每列的内存使用量,sum是对它们求和, B->KB->MB\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:#遍历每列的列名\n        col_type = df[col].dtype#列名的type\n        if col_type != object and str(col_type)!='category':#不是object也就是说这里处理的是数值类型的变量\n            c_min,c_max = df[col].min(),df[col].max() #求出这列的最大值和最小值\n            if str(col_type)[:3] == 'int':#如果是int类型的变量,不管是int8,int16,int32还是int64\n                #如果这列的取值范围是在int8的取值范围内,那就对类型进行转换 (-128 到 127)\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                #如果这列的取值范围是在int16的取值范围内,那就对类型进行转换(-32,768 到 32,767)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                #如果这列的取值范围是在int32的取值范围内,那就对类型进行转换(-2,147,483,648到2,147,483,647)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                #如果这列的取值范围是在int64的取值范围内,那就对类型进行转换(-9,223,372,036,854,775,808到9,223,372,036,854,775,807)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:#如果是浮点数类型.\n                #如果数值在float16的取值范围内,如果觉得需要更高精度可以考虑float32\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    if float16_as32:#如果数据需要更高的精度可以选择float32\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float16)  \n                #如果数值在float32的取值范围内，对它进行类型转换\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                #如果数值在float64的取值范围内，对它进行类型转换\n                else:\n                    df[col] = df[col].astype(np.float64)\n    #计算一下结束后的内存\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #相比一开始的内存减少了百分之多少\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"_uuid":"4059fd0b-eb16-46bf-8b56-f6b70bc041ae","_cell_guid":"08dd9b57-f361-43dc-9818-df7da87658dc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-12T02:58:05.221079Z","iopub.execute_input":"2025-01-12T02:58:05.221363Z","iopub.status.idle":"2025-01-12T02:58:09.809343Z","shell.execute_reply.started":"2025-01-12T02:58:05.221335Z","shell.execute_reply":"2025-01-12T02:58:09.808329Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"feature_names = [f\"feature_{i:02d}\" for i in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n\nlabel_name = 'responder_6'\nweight_name = 'weight'\ntrain_data_path = '/kaggle/input/js24-preprocessing-create-lags/training.parquet'\nvalid_data_path = '/kaggle/input/js24-preprocessing-create-lags/validation.parquet'\n\ntrain_df = pl.scan_parquet(train_data_path)\nvalid_df = pl.scan_parquet(valid_data_path)\n","metadata":{"_uuid":"f45c1db1-7650-42ef-9383-5ae4d74a8444","_cell_guid":"aff9c2ed-0d40-4579-a644-87eb299dabbf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-12T02:58:09.810307Z","iopub.execute_input":"2025-01-12T02:58:09.810941Z","iopub.status.idle":"2025-01-12T02:58:09.826420Z","shell.execute_reply.started":"2025-01-12T02:58:09.810907Z","shell.execute_reply":"2025-01-12T02:58:09.825621Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_df = train_df.collect().to_pandas()\ntrain_df = reduce_mem_usage(train_df, False)\n\nvalid_df = valid_df.collect().to_pandas()\nvalid_df = reduce_mem_usage(valid_df, False)\n\nX_train = train_df[feature_names].values\ny_train = train_df[label_name].values\nw_train = train_df[weight_name].values\nX_valid = valid_df[feature_names].values\ny_valid = valid_df[label_name].values\nw_valid = valid_df[weight_name].values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T02:58:09.826980Z","iopub.execute_input":"2025-01-12T02:58:09.827175Z","iopub.status.idle":"2025-01-12T02:59:28.523902Z","shell.execute_reply.started":"2025-01-12T02:58:09.827158Z","shell.execute_reply":"2025-01-12T02:59:28.523181Z"}},"outputs":[{"name":"stdout","text":"Memory usage of dataframe is 8119.52 MB\nMemory usage after optimization is: 4109.88 MB\nDecreased by 49.4%\nMemory usage of dataframe is 418.00 MB\nMemory usage after optimization is: 211.58 MB\nDecreased by 49.4%\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"use_es = False\nif not use_es:\n    X_train = np.concatenate((X_train,X_valid), axis=0)\n    y_train = np.concatenate((y_train,y_valid), axis=0)\n    w_train = np.concatenate((w_train,w_valid), axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T02:59:28.524699Z","iopub.execute_input":"2025-01-12T02:59:28.524901Z","iopub.status.idle":"2025-01-12T02:59:36.256943Z","shell.execute_reply.started":"2025-01-12T02:59:28.524883Z","shell.execute_reply":"2025-01-12T02:59:36.256199Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# early stop","metadata":{}},{"cell_type":"code","source":"if use_es:\n    lgb_params = {\n        'objective': 'regression',\n        \"device\"           : \"gpu\",\n        'metric': 'l2',                                      # Root Mean Squared Error\n        'boosting_type': 'gbdt',                               # Gradient Boosted Decision Trees\n        \"colsample_bytree\" : 0.8,\n        \"subsample\"        : 0.8,\n        \"num_leaves\"        : 31,\n        'learning_rate': 0.05,\n        'n_estimators':   1000,\n    }\n    model = lgb.LGBMRegressor(**lgb_params)\n    \n    model.fit(X_train, y_train, w_train,\n                          # eval_metric=[r2_lgb],\n                          eval_metric = 'l1',\n                          eval_set=[(X_valid, y_valid, w_valid)], \n                          callbacks=[\n                              lgb.early_stopping(70), \n                              lgb.log_evaluation(10)\n                          ])\n    \n    with open(f\"lgb_model_use_es.pkl\", \"wb\") as fp:\n        pickle.dump(model, fp)\nelse:\n    lgb_params = {\n        'objective': 'regression',\n        \"device\"           : \"gpu\",\n        'metric': 'l2',                                      # Root Mean Squared Error\n        'boosting_type': 'gbdt',                               # Gradient Boosted Decision Trees\n        \"colsample_bytree\" : 0.8,\n        \"subsample\"        : 0.8,\n        \"num_leaves\"        : 31,\n        'learning_rate': 0.05,\n        'n_estimators':   260,\n    }\n    model = lgb.LGBMRegressor(**lgb_params)\n    \n    model.fit(X_train, y_train, w_train,\n                          # eval_metric=[r2_lgb],\n                          eval_metric = 'l1',\n                          eval_set=[(X_valid, y_valid, w_valid)],\n             )\n    \n    with open(f\"lgb_model_no_es.pkl\", \"wb\") as fp:\n        pickle.dump(model, fp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T02:59:36.257664Z","iopub.execute_input":"2025-01-12T02:59:36.257864Z","iopub.status.idle":"2025-01-12T03:03:27.337986Z","shell.execute_reply.started":"2025-01-12T02:59:36.257846Z","shell.execute_reply":"2025-01-12T03:03:27.337260Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 21735\n[LightGBM] [Info] Number of data points in the train set: 22104280, number of used features: 88\n[LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 88 dense feature groups (1855.07 MB) transferred to GPU in 1.185340 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score -0.001785\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"if use_es:\n    XGB_Params = {\n        'learning_rate': 0.05,\n        'max_depth': 7,\n        'n_estimators': 1000,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'tree_method': 'gpu_hist',\n        'device' : 'cuda',\n        'n_gpus' : 2,\n        # 'eval_metric': 'rmse',\n        'eval_metric':'rmse',\n    }\n    \n    model = xgb.XGBRegressor(**XGB_Params)\n    \n    model.fit(X_train, y_train, sample_weight=w_train, \n                          eval_set=[(X_valid, y_valid)], \n                          sample_weight_eval_set=[w_valid], \n                          verbose=10, \n                          early_stopping_rounds=60)\n    \n    with open(f\"xgb_model_use_es.pkl\", \"wb\") as fp:\n        pickle.dump(model, fp)\n\nelse:\n    XGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 7,\n    'n_estimators': 80,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'tree_method': 'gpu_hist',\n    'device' : 'cuda',\n    'n_gpus' : 2,\n    # 'eval_metric': 'rmse',\n    'eval_metric':'rmse',\n}\n\n    model = xgb.XGBRegressor(**XGB_Params)\n    \n    model.fit(X_train, y_train, sample_weight=w_train, \n                          eval_set=[(X_valid, y_valid)], \n                          sample_weight_eval_set=[w_valid], \n                          verbose=10, )\n    \n    with open(f\"xgb_model_no_es.pkl\", \"wb\") as fp:\n        pickle.dump(model, fp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T03:03:27.340045Z","iopub.execute_input":"2025-01-12T03:03:27.340307Z","iopub.status.idle":"2025-01-12T03:07:15.420302Z","shell.execute_reply.started":"2025-01-12T03:03:27.340284Z","shell.execute_reply":"2025-01-12T03:07:15.419539Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [03:06:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [03:06:21] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_gpus\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[0]\tvalidation_0-rmse:0.73081\n[10]\tvalidation_0-rmse:0.72971\n[20]\tvalidation_0-rmse:0.72908\n[30]\tvalidation_0-rmse:0.72861\n[40]\tvalidation_0-rmse:0.72833\n[50]\tvalidation_0-rmse:0.72806\n[60]\tvalidation_0-rmse:0.72783\n[70]\tvalidation_0-rmse:0.72761\n[79]\tvalidation_0-rmse:0.72739\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [03:07:15] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"if use_es:\n    CBT_Params = {'task_type':'GPU',\n            'loss_function':'RMSE',\n            'eval_metric' : 'RMSE',\n           # 'bagging_temperature' : 0.50,\n           'iterations'          : 1000 ,\n           'learning_rate'       : 0.05,\n           'max_depth'           : 8,\n           # 'l2_leaf_reg'         : 1.25,\n           'min_data_in_leaf'    : 100,\n           # 'random_strength'     : 0.25, \n           'verbose'             : 0,\n          }\n    \n    model = cbt.CatBoostRegressor(**CBT_Params)\n    \n    evalset = cbt.Pool(X_valid, y_valid, weight=w_valid) \n    # Train CatBoost model with early stopping and verbose logging\n    model.fit(X_train, y_train, sample_weight=w_train, \n              eval_set=[evalset], \n              verbose=10, \n              early_stopping_rounds=80)\n    \n    with open(f\"cbt_model_use_es.pkl\", \"wb\") as fp:\n        pickle.dump(model, fp)\n\nelse:\n    CBT_Params = {'task_type':'GPU',\n            'loss_function':'RMSE',\n            'eval_metric' : 'RMSE',\n           # 'bagging_temperature' : 0.50,\n           'iterations'          : 315 ,\n           'learning_rate'       : 0.05,\n           'max_depth'           : 8,\n           # 'l2_leaf_reg'         : 1.25,\n           'min_data_in_leaf'    : 100,\n           # 'random_strength'     : 0.25, \n           'verbose'             : 0,\n          }\n    \n    model = cbt.CatBoostRegressor(**CBT_Params)\n    \n    evalset = cbt.Pool(X_valid, y_valid, weight=w_valid) \n    # Train CatBoost model with early stopping and verbose logging\n    model.fit(X_train, y_train, sample_weight=w_train, \n              eval_set=[evalset], \n              verbose=10, )\n    \n    with open(f\"cbt_model_no_es.pkl\", \"wb\") as fp:\n        pickle.dump(model, fp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T03:07:15.421286Z","iopub.execute_input":"2025-01-12T03:07:15.421551Z","iopub.status.idle":"2025-01-12T03:09:04.809873Z","shell.execute_reply.started":"2025-01-12T03:07:15.421528Z","shell.execute_reply":"2025-01-12T03:09:04.809025Z"}},"outputs":[{"name":"stdout","text":"0:\tlearn: 0.8292765\ttest: 0.7308430\tbest: 0.7308430 (0)\ttotal: 13.8s\tremaining: 1h 12m 12s\n10:\tlearn: 0.8273700\ttest: 0.7299705\tbest: 0.7299705 (10)\ttotal: 15.7s\tremaining: 7m 12s\n20:\tlearn: 0.8263006\ttest: 0.7295384\tbest: 0.7295384 (20)\ttotal: 17.4s\tremaining: 4m 4s\n30:\tlearn: 0.8255918\ttest: 0.7292741\tbest: 0.7292741 (30)\ttotal: 19.2s\tremaining: 2m 55s\n40:\tlearn: 0.8250007\ttest: 0.7290924\tbest: 0.7290924 (40)\ttotal: 21s\tremaining: 2m 20s\n50:\tlearn: 0.8245178\ttest: 0.7289595\tbest: 0.7289595 (50)\ttotal: 22.8s\tremaining: 1m 58s\n60:\tlearn: 0.8239851\ttest: 0.7287952\tbest: 0.7287952 (60)\ttotal: 24.7s\tremaining: 1m 42s\n70:\tlearn: 0.8235898\ttest: 0.7286631\tbest: 0.7286631 (70)\ttotal: 26.5s\tremaining: 1m 30s\n80:\tlearn: 0.8231172\ttest: 0.7285458\tbest: 0.7285458 (80)\ttotal: 28.3s\tremaining: 1m 21s\n90:\tlearn: 0.8227517\ttest: 0.7284619\tbest: 0.7284619 (90)\ttotal: 30.2s\tremaining: 1m 14s\n100:\tlearn: 0.8223580\ttest: 0.7283723\tbest: 0.7283723 (100)\ttotal: 32s\tremaining: 1m 7s\n110:\tlearn: 0.8219460\ttest: 0.7282888\tbest: 0.7282888 (110)\ttotal: 34s\tremaining: 1m 2s\n120:\tlearn: 0.8216648\ttest: 0.7282176\tbest: 0.7282176 (120)\ttotal: 35.8s\tremaining: 57.4s\n130:\tlearn: 0.8213185\ttest: 0.7281225\tbest: 0.7281225 (130)\ttotal: 37.7s\tremaining: 53s\n140:\tlearn: 0.8209193\ttest: 0.7280615\tbest: 0.7280610 (139)\ttotal: 39.6s\tremaining: 48.9s\n150:\tlearn: 0.8205087\ttest: 0.7279859\tbest: 0.7279859 (150)\ttotal: 41.5s\tremaining: 45.1s\n160:\tlearn: 0.8201895\ttest: 0.7278858\tbest: 0.7278858 (160)\ttotal: 43.5s\tremaining: 41.6s\n170:\tlearn: 0.8199047\ttest: 0.7278089\tbest: 0.7278089 (170)\ttotal: 45.4s\tremaining: 38.2s\n180:\tlearn: 0.8195473\ttest: 0.7277141\tbest: 0.7277141 (180)\ttotal: 47.3s\tremaining: 35s\n190:\tlearn: 0.8191794\ttest: 0.7276442\tbest: 0.7276442 (190)\ttotal: 49.2s\tremaining: 31.9s\n200:\tlearn: 0.8188042\ttest: 0.7275756\tbest: 0.7275756 (200)\ttotal: 51.1s\tremaining: 29s\n210:\tlearn: 0.8184886\ttest: 0.7275219\tbest: 0.7275219 (210)\ttotal: 52.9s\tremaining: 26.1s\n220:\tlearn: 0.8179831\ttest: 0.7274321\tbest: 0.7274321 (220)\ttotal: 54.9s\tremaining: 23.3s\n230:\tlearn: 0.8175934\ttest: 0.7273400\tbest: 0.7273400 (230)\ttotal: 56.7s\tremaining: 20.6s\n240:\tlearn: 0.8173459\ttest: 0.7272614\tbest: 0.7272614 (240)\ttotal: 58.6s\tremaining: 18s\n250:\tlearn: 0.8171568\ttest: 0.7272012\tbest: 0.7272012 (250)\ttotal: 1m\tremaining: 15.4s\n260:\tlearn: 0.8168925\ttest: 0.7271280\tbest: 0.7271280 (260)\ttotal: 1m 2s\tremaining: 12.9s\n270:\tlearn: 0.8166313\ttest: 0.7270776\tbest: 0.7270776 (270)\ttotal: 1m 4s\tremaining: 10.4s\n280:\tlearn: 0.8163688\ttest: 0.7270275\tbest: 0.7270275 (280)\ttotal: 1m 6s\tremaining: 7.99s\n290:\tlearn: 0.8161154\ttest: 0.7269744\tbest: 0.7269744 (290)\ttotal: 1m 7s\tremaining: 5.59s\n300:\tlearn: 0.8159043\ttest: 0.7269167\tbest: 0.7269167 (300)\ttotal: 1m 9s\tremaining: 3.24s\n310:\tlearn: 0.8156329\ttest: 0.7268663\tbest: 0.7268663 (310)\ttotal: 1m 11s\tremaining: 920ms\n314:\tlearn: 0.8154963\ttest: 0.7268425\tbest: 0.7268425 (314)\ttotal: 1m 12s\tremaining: 0us\nbestTest = 0.7268424923\nbestIteration = 314\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# no early stop","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Define Parameters\n# train_data = lgb.Dataset(X_train, label=y_train, weight=w_train)\n# valid_data = lgb.Dataset(X_valid, label=y_valid, weight=w_valid, reference=train_data)\n\n# lgb_params = {\n#     'objective': 'regression',\n#     \"device\"           : \"gpu\",\n#     'metric': 'l2',                                      # Root Mean Squared Error\n#     'boosting_type': 'gbdt',                               # Gradient Boosted Decision Trees\n#     \"colsample_bytree\" : 0.8,\n#     \"subsample\"        : 0.8,\n#     \"num_leaves\"        : 31,\n#     'learning_rate': 0.05,\n#     'n_estimators':   1000,\n# }\n\n# # Train the model\n# lgbm_model = lgb.train(\n#     params,\n#     train_data,\n#     valid_sets=[train_data, valid_data],\n#     callbacks=[\n#                 lgb.early_stopping(100), \n#                 lgb.log_evaluation(10)],\n# )\n\n# lgbm_model.save_model(f\"lgbm_model_offline.json\")\n\n# if valid_data is not None:\n#     y_pred_valid = lgbm_model.predict(X_valid)\n#     valid_score = r2_score(y_valid, y_pred_valid, sample_weight=w_valid )\n#     print('valid score:',valid_score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T03:09:04.810762Z","iopub.execute_input":"2025-01-12T03:09:04.811101Z","iopub.status.idle":"2025-01-12T03:09:04.814650Z","shell.execute_reply.started":"2025-01-12T03:09:04.811074Z","shell.execute_reply":"2025-01-12T03:09:04.813838Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def get_model(model_name, n_estimators=None):\n#     # XGBoost parameters\n    \n#     XGB_Params = {\n#         'learning_rate': 0.05,\n#         'max_depth': 7,\n#         'n_estimators': 1000 if n_estimators is None else n_estimators,\n#         'subsample': 0.8,\n#         'colsample_bytree': 0.8,\n#         'reg_alpha': 1,\n#         'reg_lambda': 2,\n#         'random_state': 2024,\n#         'tree_method': 'gpu_hist',\n#         'device' : 'cuda',\n#         'n_gpus' : 2,\n#         # 'eval_metric': 'rmse',\n#         'eval_metric':r2_xgb,\n#     }\n\n#     LGB_Params={\"device\"           : \"gpu\",\n#              \"objective\"        : \"regression_l2\",\n#              \"metrics\"          : \"custom\",\n#              \"n_estimators\"     :  1000 if n_estimators is None else n_estimators,\n#              \"max_depth\"        : 8,\n#              \"learning_rate\"    : 0.05,\n#              \"colsample_bytree\" : 0.8,\n#              \"subsample\"        : 0.80,\n#              \"random_state\"     : 2024,\n#              \"reg_alpha\"        : 0.1,\n#              \"reg_lambda\"       : 1.0,\n#              \"verbosity\"        : -1,\n#          }\n    \n#     CBT_Params = {'task_type':'GPU',\n#            'random_state':2024,\n#            'eval_metric'         : r2_cbt(),\n#             'loss_function':'RMSE',\n#             # 'eval_metric' : 'RMSE',\n#            # 'bagging_temperature' : 0.50,\n#            'iterations'          : 1000 if n_estimators is None else n_estimators,\n#            'learning_rate'       : 0.05,\n#            'max_depth'           : 8,\n#            'l2_leaf_reg'         : 1.25,\n#            # 'min_data_in_leaf'    : 24,\n#            # 'random_strength'     : 0.25, \n#            'verbose'             : 0,\n#           }\n    \n#     if model_name == 'xgb':\n#         model = xgb.XGBRegressor(**XGB_Params)\n#     if model_name == 'lgb':\n#         model = lgb.LGBMRegressor(**LGB_Params)\n#     if model_name == 'cbt':\n#         model = cbt.CatBoostRegressor(**CBT_Params)\n#     return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T03:09:04.815498Z","iopub.execute_input":"2025-01-12T03:09:04.815864Z","iopub.status.idle":"2025-01-12T03:09:04.834766Z","shell.execute_reply.started":"2025-01-12T03:09:04.815841Z","shell.execute_reply":"2025-01-12T03:09:04.833918Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\n\n\n# def train_model(train_data, valid_data=None,model_name='lgb',n_estimators=None ):\n    \n#     # Get the model from the dictionary\n#     model = get_model(model_name, n_estimators)\n    \n#     X_train, y_train, w_train = train_data\n#     if valid_data is not None:\n#         X_valid, y_valid, w_valid = valid_data\n        \n#     # Train the model based on the type (LightGBM, XGBoost, or CatBoost)\n#     if model_name == 'lgb':\n#         # Train LightGBM model with early stopping and evaluation logging\n#         if valid_data is not None:\n#             model.fit(X_train, y_train, w_train,\n#                       # eval_metric=[r2_lgb],\n#                       eval_metric = 'l1',\n#                       eval_set=[(X_valid, y_valid, w_valid)], \n#                       callbacks=[\n#                           lgb.early_stopping(100), \n#                           lgb.log_evaluation(10)\n#                       ])\n#         else:\n#             model.fit(X_train,y_train,sample_weight=w_train)\n        \n#     elif model_name == 'cbt':\n#         # Prepare evaluation set for CatBoost\n#         if valid_data is not None:\n#             evalset = cbt.Pool(X_valid, y_valid, weight=w_valid)\n            \n#             # Train CatBoost model with early stopping and verbose logging\n#             model.fit(X_train, y_train, sample_weight=w_train, \n#                       eval_set=[evalset], \n#                       verbose=10, \n#                       early_stopping_rounds=100)\n#         else:\n#             model.fit(X_train,y_train,sample_weight=w_train)\n        \n#     else:\n#         # Train XGBoost model with early stopping and verbose logging\n#         if valid_data is not None:\n#             model.fit(X_train, y_train, sample_weight=w_train, \n#                       eval_set=[(X_valid, y_valid)], \n#                       sample_weight_eval_set=[w_valid], \n#                       verbose=10, \n#                       early_stopping_rounds=100)\n#         else:\n#             model.fit(X_train,y_train,sample_weight=w_train)\n        \n#     if valid_data is not None:\n#         y_pred_valid = model.predict(X_valid)\n#         valid_score = r2_score(y_valid, y_pred_valid, sample_weight=w_valid )\n#         print('valid score:',valid_score)\n        \n#     # # Save the trained model to a file\n#     # joblib.dump(model, f'./models/{model_name}_train{train_date_start}-{train_date_end}_valid{valid_date_start}-{valid_date_end}_score-{valid_score:.6f}.model')\n#     with open(f\"{model_name}.pkl\", \"wb\") as fp:\n#             pickle.dump(model, fp)\n\n\n","metadata":{"_uuid":"fea17525-cc00-4b8a-848d-b2284abfff32","_cell_guid":"366c2809-0454-4b02-a2ad-09eef893bd00","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-12T03:09:04.835708Z","iopub.execute_input":"2025-01-12T03:09:04.835998Z","iopub.status.idle":"2025-01-12T03:09:04.852532Z","shell.execute_reply.started":"2025-01-12T03:09:04.835970Z","shell.execute_reply":"2025-01-12T03:09:04.851723Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# model_name = 'cbt'\n# num_trees = None\n\n# train_model(train_data, valid_data, model_name, num_trees)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T03:09:04.853391Z","iopub.execute_input":"2025-01-12T03:09:04.853697Z","iopub.status.idle":"2025-01-12T03:09:04.870219Z","shell.execute_reply.started":"2025-01-12T03:09:04.853669Z","shell.execute_reply":"2025-01-12T03:09:04.869411Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
