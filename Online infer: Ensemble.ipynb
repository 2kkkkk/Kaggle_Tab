{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":9801075,"sourceType":"datasetVersion","datasetId":6006872},{"sourceId":9806342,"sourceType":"datasetVersion","datasetId":6010899},{"sourceId":9838282,"sourceType":"datasetVersion","datasetId":6035149},{"sourceId":10292029,"sourceType":"datasetVersion","datasetId":6369558},{"sourceId":10386524,"sourceType":"datasetVersion","datasetId":6434541},{"sourceId":10401248,"sourceType":"datasetVersion","datasetId":6444482},{"sourceId":10442114,"sourceType":"datasetVersion","datasetId":6463198},{"sourceId":10447358,"sourceType":"datasetVersion","datasetId":6466722},{"sourceId":203900450,"sourceType":"kernelVersion"},{"sourceId":216602411,"sourceType":"kernelVersion"}],"dockerImageVersionId":30823,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os,gc\nimport glob\nimport numpy as np\nimport time\nimport pandas as pd\nimport polars as pl\nimport lightgbm as lgb\nimport xgboost as xgb\nimport pickle\nimport kaggle_evaluation.jane_street_inference_server\nfrom sklearn.linear_model import Ridge\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport tqdm\nfrom pytorch_lightning import (LightningDataModule, LightningModule, Trainer)\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer","metadata":{"_uuid":"b4ae5dd0-d456-4603-a6fd-a4538c015c13","_cell_guid":"ac2781e7-c72c-4560-8d02-a287ff0ac873","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-01-13T10:43:57.967340Z","iopub.execute_input":"2025-01-13T10:43:57.967585Z","iopub.status.idle":"2025-01-13T10:44:08.277003Z","shell.execute_reply.started":"2025-01-13T10:43:57.967563Z","shell.execute_reply":"2025-01-13T10:44:08.276162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CONFIG:\n    seed = 42\n    target_name = \"responder_6\"\n    weight_name = 'weight'\n    feature_names = [f\"feature_{i:02d}\" for i in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n    lag_cols_rename = { f\"responder_{idx}\" : f\"responder_{idx}_lag_1\" for idx in range(9)}\n    ridge_features = [item for item in feature_names if item not in ['feature_09','feature_10','feature_11']]\n    \n    DEBUG = False\n\n    retrain_every_n_days = 20\n    retrain_after_n_days = 20\n    retrain_last_n_days_data = 250\n\n    cache_cols = ['date_id', 'time_id', 'symbol_id', 'weight', 'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'responder_0_lag_1', 'responder_1_lag_1', 'responder_2_lag_1', 'responder_3_lag_1', 'responder_4_lag_1', 'responder_5_lag_1', 'responder_6_lag_1', 'responder_7_lag_1', 'responder_8_lag_1']\n    newmodel_lr = 0.1\n    newmodel_num_boost_round = 50\n\n    retrain_feature_num = 45","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:44:08.277709Z","iopub.execute_input":"2025-01-13T10:44:08.278168Z","iopub.status.idle":"2025-01-13T10:44:08.284654Z","shell.execute_reply.started":"2025-01-13T10:44:08.278146Z","shell.execute_reply":"2025-01-13T10:44:08.283727Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# xgb chinese version","metadata":{}},{"cell_type":"code","source":"lag_ndays = 2\n\nhistory = pl.scan_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet\").select(['date_id','time_id','symbol_id'] + [f\"responder_{idx}\" for idx in range(9)]).filter(\n    (pl.col(\"date_id\")>=(1698 - lag_ndays))&(pl.col(\"date_id\")<1698)\n)\n# 这里将历史date_id变为从-N到-1, 假设test的date_id=0紧随train的date_id=1698,\n# 在第一个batch给出的lags应该是date_id=1698的responser(但date_id给的0),\n# 这样history中最后一个date_id=1697变为-1, 正好可以和推理时给的lags衔接上\nhistory = history.with_columns(\n    date_id = (pl.col(\"date_id\") - pl.lit(1698)).cast(pl.Int16)\n)\nhistory = history.collect()\n\n# 这里是为了统一特征的dtypes(polars在concat时如果dtype对不上会报错)\nhistory_column_types = {\n    'date_id': pl.Int16,\n    'time_id': pl.Int16,\n    'symbol_id': pl.Int16\n}\nfeature_column_types = {}\nfor f in [f\"feature_{idx:02d}\" for idx in range(79)]:\n    feature_column_types[f] = pl.Float32\n\nresponder_column_types = {}\nfor f in [f\"responder_{idx}\" for idx in range(9)]:\n    responder_column_types[f] = pl.Float32\n\nhistory = history.cast(history_column_types)\nhistory = history.cast(responder_column_types)\n\nmodel_path = \"/kaggle/input/janestreet-public-model/xgb_001.pkl\"\nwith open(model_path, \"rb\") as fp:\n    result = pickle.load(fp)\nmodel_pub_xgb_cn_ver = result[\"model\"]\nfeatures_pub_xgb_cn_ver = result[\"features\"]\n\ndef predict_public_xgb_chinese_version(test: pl.DataFrame, lags: pl.DataFrame | None):\n    global history\n    global lags_infer\n    lag_cols_rename = { f\"responder_{idx}_lag_1\" : f\"responder_{idx}\" for idx in range(9)}\n    lag_target_cols_name = [f\"responder_{idx}\" for idx in range(9)]\n    lag_cols_original = [\"date_id\", \"time_id\", \"symbol_id\"] + [f\"responder_{idx}\" for idx in range(9)]\n    def create_agg_list(day, columns):\n        agg_mean_list = [pl.col(c).mean().name.suffix(f\"_mean_{day}d\") for c in columns]\n        agg_std_list = [pl.col(c).std().name.suffix(f\"_std_{day}d\") for c in columns]\n        agg_max_list = [pl.col(c).max().name.suffix(f\"_max_{day}d\") for c in columns]\n        agg_last_list = [pl.col(c).last().name.suffix(f\"_last_{day}d\") for c in columns]\n        agg_list = agg_mean_list + agg_std_list + agg_max_list + agg_last_list\n        return agg_list\n\n\n    symbol_ids = test.select(\"symbol_id\").to_numpy()[:, 0]\n    current_date = test.select(\"date_id\").to_numpy()[:, 0][0]\n\n    if lags is not None:\n        # 原始lags先存储到history更新历史数据\n        lags = lags.rename(lag_cols_rename)\n        lags = lags.cast(history_column_types)\n        lags = lags.cast(responder_column_types)\n        history = pl.concat([history, lags])\n        \n        # 只储存最近N天的历史数据\n        history = history.filter(pl.col(\"date_id\") > (current_date - lag_ndays))\n        # 这里用的XGB模型只使用了shift 1天的统计值\n        agg_list = create_agg_list(1, lag_target_cols_name)\n        shift_n_data = history.filter(pl.col(\"date_id\") == current_date)\n        lags_infer = shift_n_data.group_by([\"date_id\", \"symbol_id\"], maintain_order=True).agg(agg_list)\n  \n    test = test.cast(history_column_types)\n    test = test.cast(feature_column_types)\n    # 在一个date_id下的所有batch用到的lags_infer是相同的\n    # 像lags_infer这样的统计特征在每个date_id的time_id=0时构造完成\n    X_test = test.join(lags_infer, on=[\"date_id\", \"symbol_id\"], how=\"left\")\n    \n    preds = np.zeros((X_test.shape[0],))\n    preds += model_pub_xgb_cn_ver.predict(X_test[features_pub_xgb_cn_ver].to_pandas().values)\n    return preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:44:08.285589Z","iopub.execute_input":"2025-01-13T10:44:08.285887Z","iopub.status.idle":"2025-01-13T10:44:08.954393Z","shell.execute_reply.started":"2025-01-13T10:44:08.285859Z","shell.execute_reply":"2025-01-13T10:44:08.953727Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# xgb motono","metadata":{}},{"cell_type":"code","source":"def predict_public_xgb_motono(test_with_lag_features: pl.DataFrame):\n    xgb_model = None\n    with open( \"/kaggle/input/js-with-lags-trained-xgb/result.pkl\", \"rb\") as fp:\n        result = pickle.load(fp)\n        xgb_model = result[\"model\"]\n    \n    xgb_feature_cols = [\"symbol_id\", \"time_id\"] + CONFIG.feature_names\n    \n    preds = np.zeros((test_with_lag_features.shape[0],))\n    preds += xgb_model.predict(test_with_lag_features[xgb_feature_cols].to_pandas())\n    return preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:44:08.956044Z","iopub.execute_input":"2025-01-13T10:44:08.956274Z","iopub.status.idle":"2025-01-13T10:44:08.963137Z","shell.execute_reply.started":"2025-01-13T10:44:08.956253Z","shell.execute_reply":"2025-01-13T10:44:08.962426Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# nn","metadata":{}},{"cell_type":"code","source":"class NN(LightningModule):\n    def __init__(self, input_dim, hidden_dims, dropouts, lr, weight_decay):\n        super().__init__()\n        self.save_hyperparameters()\n        layers = []\n        in_dim = input_dim\n        for i, hidden_dim in enumerate(hidden_dims):\n            layers.append(nn.BatchNorm1d(in_dim))\n            if i > 0:\n                layers.append(nn.SiLU())\n            if i < len(dropouts):\n                layers.append(nn.Dropout(dropouts[i]))\n            layers.append(nn.Linear(in_dim, hidden_dim))\n            # layers.append(nn.ReLU())\n            in_dim = hidden_dim\n        layers.append(nn.Linear(in_dim, 1)) \n        layers.append(nn.Tanh())\n        self.model = nn.Sequential(*layers)\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.validation_step_outputs = []\n\n    def forward(self, x):\n        return 5 * self.model(x).squeeze(-1)  \n\n    def training_step(self, batch):\n        x, y, w = batch\n        y_hat = self(x)\n        loss = F.mse_loss(y_hat, y, reduction='none') * w  #\n        loss = loss.mean()\n        self.log('train_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n        return loss\n\n    def validation_step(self, batch):\n        x, y, w = batch\n        y_hat = self(x)\n        loss = F.mse_loss(y_hat, y, reduction='none') * w\n        loss = loss.mean()\n        self.log('val_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n        self.validation_step_outputs.append((y_hat, y, w))\n        return loss\n\n    def on_validation_epoch_end(self):\n        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n        if self.trainer.sanity_checking:\n            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n        else:\n            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n            # r2_val\n            val_r_square = r2_val(y, prob, weights)\n            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n        self.validation_step_outputs.clear()\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n                                                               verbose=True)\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': {\n                'scheduler': scheduler,\n                'monitor': 'val_loss',\n            }\n        }\n\n    def on_train_epoch_end(self):\n        if self.trainer.sanity_checking:\n            return\n        epoch = self.trainer.current_epoch\n        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n        print(f\"Epoch {epoch}: {formatted_metrics}\")\n\nN_folds = 5\nnn_models = []\ndevice = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n\nnn_features = [item for item in CONFIG.feature_names if item not in ['feature_09','feature_10','feature_11']]\nnn_model_root_path = '/kaggle/input/js-my-offline-nn-model-5fold'\n\n# nn_features = [item for item in CONFIG.feature_names]\n# nn_model_root_path = '/kaggle/input/js-xs-nn-trained-model'\n\nfor checkpoint_path in glob.glob(nn_model_root_path + r'/*'):\n    model = NN.load_from_checkpoint(checkpoint_path)\n    nn_models.append(model.to(\"cuda:0\"))\n\ndef predict_nn(test_with_lag_features: pl.DataFrame):\n    # fill na 0\n    test_input = test_with_lag_features[nn_features].to_pandas()\n    test_input = test_input.fillna(0)\n    preds = np.zeros((test_input.shape[0],))\n    test_input = torch.FloatTensor(test_input.values).to(device)\n    with torch.no_grad():\n        for i, nn_model in enumerate(nn_models):\n            nn_model.eval()\n            preds += nn_model(test_input).cpu().numpy() / len(nn_models)\n\n    return preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:44:08.964536Z","iopub.execute_input":"2025-01-13T10:44:08.964766Z","iopub.status.idle":"2025-01-13T10:44:10.248406Z","shell.execute_reply.started":"2025-01-13T10:44:08.964746Z","shell.execute_reply":"2025-01-13T10:44:10.247766Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# my xgb lgb cat","metadata":{}},{"cell_type":"code","source":"my_gbdt_models = []\nmy_gbdt_feature_names = [f\"feature_{i:02d}\" for i in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n\nfor file in glob.glob('/kaggle/input/js-xgb-lgb-cat-offline-model-2025-1-12/*model*'):\n    \n    print(file)\n    with open(file,'rb') as fp:\n        my_gbdt_models.append(pickle.load(fp))\n\ndef predict_my_offline_gbdt(test_with_lag_features: pl.DataFrame):\n    preds = np.zeros((test_with_lag_features.shape[0],))\n    \n    feat = test_with_lag_features[my_gbdt_feature_names].to_numpy()\n    \n    pred = [model.predict(feat) for model in my_gbdt_models]\n    pred = np.mean(pred, axis=0)\n    pred = np.clip(pred, a_min=-5, a_max=5)\n    return preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:44:10.249123Z","iopub.execute_input":"2025-01-13T10:44:10.249342Z","iopub.status.idle":"2025-01-13T10:44:10.800618Z","shell.execute_reply.started":"2025-01-13T10:44:10.249322Z","shell.execute_reply":"2025-01-13T10:44:10.799686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load model\noffline_lgbm_path = '/kaggle/input/lgbm-offline-model/lgbm_model_offline_450iter.json'\nlgbm_model = lgb.Booster(model_file=offline_lgbm_path)\nfeatures_retrain_indexs = np.argsort(lgbm_model.feature_importance())[::-1][:CONFIG.retrain_feature_num]\n\noffline_cache_path = '/kaggle/input/data-create-create-lags/offline_cache.parquet'\noffline_label_path = '/kaggle/input/data-create-create-lags/offline_labels.parquet'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:44:10.801494Z","iopub.execute_input":"2025-01-13T10:44:10.801712Z","iopub.status.idle":"2025-01-13T10:44:10.856045Z","shell.execute_reply.started":"2025-01-13T10:44:10.801692Z","shell.execute_reply":"2025-01-13T10:44:10.855464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    'objective': 'regression',\n    \"device\"           : \"gpu\",\n    'metric': 'l2',                                      # Root Mean Squared Error\n    'boosting_type': 'gbdt',                               # Gradient Boosted Decision Trees\n    \"colsample_bytree\" : 0.8,\n    \"subsample\"        : 0.8,\n    \"num_leaves\"        : 31,\n    # \"reg_alpha\"        : 0.1,\n    # \"reg_lambda\"       : 1.0,\n    'learning_rate': CONFIG.newmodel_lr,\n    'n_estimators':   CONFIG.newmodel_num_boost_round,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:44:10.856512Z","iopub.execute_input":"2025-01-13T10:44:10.856714Z","iopub.status.idle":"2025-01-13T10:44:10.860435Z","shell.execute_reply.started":"2025-01-13T10:44:10.856695Z","shell.execute_reply":"2025-01-13T10:44:10.859828Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# predict function","metadata":{}},{"cell_type":"code","source":"# Initialize global vars\ncache = None\ncache_list = []\n# tot nb of days counter\nday_count = 0\n# training counter to be reset after each train\ntrain_counter = 0\nnew_lgbm_model = None\nonline_ridge_model = None\nlags_ : pl.DataFrame | None = None\nlabels : pl.DataFrame | None = None\n\nlags_last : pl.DataFrame | None = None\n\ncache = pl.scan_parquet(offline_cache_path).filter(pl.col(\"date_id\").gt(-300)).collect()\nlabels = pl.scan_parquet(offline_label_path).filter(pl.col(\"date_id\").gt(-300)).collect()\nid_column_types = {\n    'date_id': pl.Int16,\n    'time_id': pl.Int16,\n    'symbol_id': pl.Int16\n}\ncache = cache.cast(id_column_types)\nlabels = labels.cast(id_column_types)\n\n# Each batch of predictions (except the very first) must be returned within 1 minute of the batch features being provided.\ndef predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n    \"\"\"Make a prediction.\"\"\"\n    # All the responders from the previous day are passed in at time_id == 0. We save them in a global variable for access at every time_id.\n    # Use them as extra features, if you like.\n    global cache          # Declare the global cache\n    global day_count\n    global new_lgbm_model\n    global lgbm_model\n    global lags_\n    global cache_list\n    global labels\n    global train_counter\n    global lags_last\n    global online_ridge_model\n\n    # st_time = time.time()\n    pred_public_xgb_chinese_ver = predict_public_xgb_chinese_version(test.clone(),lags)\n    # print('预测xgb耗时:',time.time()-st_time)\n    \n    id_column_types = {\n        'date_id': pl.Int16,\n        'time_id': pl.Int16,\n        'symbol_id': pl.Int16\n    }\n    test = test.cast(id_column_types)\n    if lags is not None:\n        lags = lags.cast(id_column_types)\n    \n    # Store lags, since they are provided at the start of each day but no for other tiòe_id of the same day\n    if lags is not None:\n        # print(f\"Day count: {day_count}\")\n        lags_ = lags\n        day_count += 1\n        train_counter += 1\n        # store ground truth from previous day\n        update_labels = lags_[\"date_id\", \"symbol_id\", \"time_id\",\"responder_6_lag_1\"]\n        lag_cols_rename = {\"responder_6_lag_1\": \"responder_6\"}\n        update_labels = update_labels.rename(lag_cols_rename)\n        if labels is not None:\n            labels = pl.concat([labels, update_labels], rechunk=True)\n        else:\n            labels = update_labels\n\n    # Init prediction\n    predictions = test.select(\n        'row_id',\n        pl.lit(0.0).alias('responder_6'),\n    )\n\n    # join lag feature\n    if lags is not None:\n        lags_last = lags.group_by([\"date_id\", \"symbol_id\"], maintain_order=True).last() # pick up last record of previous date\n        lags_last = lags_last.drop([\"time_id\"])\n    test = test.join(lags_last, on=[\"date_id\", \"symbol_id\"],  how=\"left\")\n    \n    #----------------------xgb motono--------------\n    pred_public_xgb_motono = predict_public_xgb_motono(test)\n    pred_nn = predict_nn(test)\n    # pred_my_gbdt = predict_my_offline_gbdt(test)\n    \n    cache_list.append(test)\n\n    # initialize preds\n    preds = np.zeros((test.shape[0],))\n\n    # ======================== retrain part ====================================================\n    # re-train a model on the fly every N days\n    if train_counter % CONFIG.retrain_every_n_days == 0 and day_count>=CONFIG.retrain_after_n_days:\n        print(\"Start retraining\")\n        # 更新cache 1.合并旧cache和cache_update 2.保留cache最新的time_window天\n        if cache is not None:\n            print('cache不是none')\n            cache_update = pl.concat(cache_list, rechunk=True)\n            cache_update = cache_update.select(CONFIG.cache_cols)\n            cache = cache.select(CONFIG.cache_cols)\n            cache = cache.cast(id_column_types)\n            cache_update = cache_update.cast(id_column_types)\n\n            # print(cache_update.columns)\n            cache = pl.concat([cache, cache_update], rechunk=True)\n        else:\n            print('cache是none！！！！')\n            cache = pl.concat(cache_list, rechunk=True)\n        # store only last time_window days\n        print('cache最小最大date_id',np.min(cache[\"date_id\"].to_numpy()),np.max(cache[\"date_id\"].to_numpy()))\n        days = np.sort(np.unique(cache[\"date_id\"].to_numpy()))\n        days = days[-CONFIG.retrain_last_n_days_data:]\n        min_day = np.min(days)\n        print('cache最小天：', min_day)\n        cache = cache.filter(pl.col(\"date_id\") >= min_day)\n        print('cache shape:',cache.shape)\n\n        # filter labels\n        # move data back to the previous day (we receive the lags at the same day but they are the ground truth of the previous day)\n        df = labels.with_columns(\n            (pl.col(\"date_id\") -1).alias(\"date_id\")\n        )\n        df = df.filter(pl.col(\"date_id\") >= np.min(cache[\"date_id\"].to_numpy()))\n        \n        # prepare data for training\n        train = cache.join(df, on=[\"date_id\", \"symbol_id\", \"time_id\"],  how=\"left\")\n        X_train = train[CONFIG.feature_names].to_numpy()[:,features_retrain_indexs]\n        y_train = train[CONFIG.target_name].to_numpy().flatten()\n        w_train = train[CONFIG.weight_name].to_numpy().flatten()\n\n        # 取最后20w的数据作为valid\n        # valid_data_num = 200000\n        # X_valid = X_train[-valid_data_num:]\n        # y_valid = y_train[-valid_data_num:]\n        # w_valid = w_train[-valid_data_num:]\n        \n        # Re-train the model\n        start_tim = time.time()\n        train_data = lgb.Dataset(X_train, label=y_train)\n        del X_train,y_train,w_train\n        gc.collect()\n        # valid_data = lgb.Dataset(X_valid, label=y_valid, weight=w_valid, reference=train_data)\n        print('构建lgb dataset耗时：',time.time()-start_tim)\n        # Re-train the model\n        # new_lgbm_model = lgb.train(\n        #     params,\n        #     train_data,\n        #     valid_sets=[train_data, valid_data],\n        #     callbacks=[\n        #         lgb.early_stopping(25), \n        #         lgb.log_evaluation(50)],\n        # )\n        new_lgbm_model = lgb.train(\n            params,\n            train_data,\n            num_boost_round= CONFIG.newmodel_num_boost_round,\n        )\n\n        # train ridge online model\n        X_train_ridge =  train[CONFIG.ridge_features].to_pandas().fillna(0).values\n        y_train_ridge = train[CONFIG.target_name].to_pandas().fillna(0).values.flatten()\n        del train\n        gc.collect()\n        online_ridge_model = Ridge()\n        online_ridge_model.fit(X_train_ridge,y_train_ridge)\n        \n        # reset counter otherwise we will retrain for each time_id of the same day\n        train_counter = 1\n        # empty cache list\n        cache_list = []\n\n\n    # ========================retrain part end ====================================================\n    \n    y_pred_offline = 0.325*pred_public_xgb_chinese_ver + 0.175*pred_public_xgb_motono + 0.5*pred_nn\n    X = test[CONFIG.feature_names].to_numpy()\n    if new_lgbm_model:\n        y_pred_online = new_lgbm_model.predict(X[:,features_retrain_indexs],num_iteration=new_lgbm_model.best_iteration)\n        if online_ridge_model:\n            y_pred_ridge = online_ridge_model.predict(test[CONFIG.ridge_features].to_pandas().fillna(0).values)\n            y_pred_online = 0.8*y_pred_online + 0.2*y_pred_ridge\n        y_pred = (0.45*y_pred_online+0.55*y_pred_offline)\n    else:\n        y_pred = y_pred_offline\n\n    preds = y_pred\n    # print(f\"predict> preds.shape =\", preds.shape)\n    \n    predictions = \\\n    test.select('row_id').\\\n    with_columns(\n        pl.Series(\n            name   = 'responder_6', \n            values = np.clip(preds, a_min = -5, a_max = 5),\n            dtype  = pl.Float64,\n        )\n    )\n\n    if isinstance(predictions, pl.DataFrame):\n        assert predictions.columns == ['row_id', 'responder_6']\n    elif isinstance(predictions, pd.DataFrame):\n        assert (predictions.columns == ['row_id', 'responder_6']).all()\n    else:\n        raise TypeError('The predict function must return a DataFrame')\n    # Confirm has as many rows as the test data.\n    assert len(predictions) == len(test)\n\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:44:10.861133Z","iopub.execute_input":"2025-01-13T10:44:10.861762Z","iopub.status.idle":"2025-01-13T10:44:36.909130Z","shell.execute_reply.started":"2025-01-13T10:44:10.861738Z","shell.execute_reply":"2025-01-13T10:44:36.908414Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# debug","metadata":{}},{"cell_type":"code","source":"def makelag(date_id):\n    \"\"\"\n    Making lag at the previout day\n\n    Args:\n    date_id (int): date_id at the previout day\n    \n    Returns:\n    pl.dataframe\n    \"\"\"\n    responder_cols = [s for s in train.columns if \"responder\" in s]\n    lag = alltraindata.filter(pl.col(\"date_id\")==date_id).select([\"date_id\",\"time_id\",\"symbol_id\"] + responder_cols).collect()\n    lag.columns = lag_sample.columns\n    \n    return lag\n\ndef weighted_zero_mean_r2(y_true, y_pred, weights):\n    \"\"\"\n    Calculate the sample weighted zero-mean R-squared score.\n\n    Parameters:\n    y_true (numpy.ndarray): Ground-truth values for responder_6.\n    y_pred (numpy.ndarray): Predicted values for responder_6.\n    weights (numpy.ndarray): Sample weight vector.\n\n    Returns:\n    float: The weighted zero-mean R-squared score.\n    \"\"\"\n    numerator = np.sum(weights * (y_true - y_pred)**2)\n    denominator = np.sum(weights * y_true**2)\n    \n    r2_score = 1 - numerator / denominator\n    return r2_score\n\n\nif CONFIG.DEBUG:\n    lag_sample = pl.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet/date_id=0/part-0.parquet\")\n    alltraindata = pl.scan_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet\")\n    # pick 50 days\n    nb_days = 100\n    train = alltraindata.filter(pl.col(\"date_id\")>1698-nb_days).collect()\n    train = train.with_columns(pl.Series(range(len(train))).alias(\"row_id\"))\n\n\n## Step 1 The data is split by day using group_by.\nif CONFIG.DEBUG:\n    all_submission_dataframe = []\n    # Initialize global vars\n    # cache = None\n    cache_list = []\n    # tot nb of days counter\n    day_count = 0\n    # training counter to be reset after each train\n    train_counter = 0\n    new_lgbm_model = None\n    lags_ : pl.DataFrame | None = None\n    # labels : pl.DataFrame | None = None\n    for num_days, df_per_day in train.group_by(\"date_id\",maintain_order=True):\n        \n        ## Step 2 The data is split by time_id using group_by, and the lag is generated (for time_id == 0).\n        \n        for time_id, test in df_per_day.group_by(\"time_id\",maintain_order=True):\n            \n            ## when time_id == 0, makelags\n            \n            if time_id[0] == 0:\n                lag = makelag(num_days[0] - 1)\n            else:\n                lag = None\n            start_time = time.time()\n            submission_dataframe = predict(test, lag)\n            elapsed = time.time() - start_time\n            if elapsed>10:\n                print(num_days,time_id)\n                print('耗时：{}'.format(elapsed))\n            all_submission_dataframe.append(submission_dataframe)\n            \n    all_submission_dataframe = pl.concat(all_submission_dataframe)\n    all_submission_dataframe\n\n    print(weighted_zero_mean_r2(train.select(\"responder_6\").to_numpy().reshape(-1), all_submission_dataframe.select(\"responder_6\").to_numpy().reshape(-1), train.select(\"weight\").to_numpy().reshape(-1)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T10:44:36.909939Z","iopub.execute_input":"2025-01-13T10:44:36.910238Z","execution_failed":"2025-01-13T10:47:47.279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n        )\n    )","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-13T10:47:47.281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}